{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CSC3206 Artificial Intelligence This site hosts the lab sheets for the module of CSC3206 Artificial Intelligence in the Department of Computing and Information Systems (DCIS) in Sunway University. Aim The aim of these labs is to guide the students to implement the basic artificial intelligence (AI) algorithms with and/or without Python libraries. Information The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab. Schedule The schedule is subject to change. Week 2 Getting started Week 2 Lab 1 Week 4 Lab 2 Week 6 Lab 3 Week 7 Lab 4 Week 9 Lab 5 Week 10 Lab 6 Week 11 Lab 7 Week 12 Lab 8 Week 13 Lab 9","title":"Overview"},{"location":"#aim","text":"The aim of these labs is to guide the students to implement the basic artificial intelligence (AI) algorithms with and/or without Python libraries.","title":"Aim"},{"location":"#information","text":"The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab.","title":"Information"},{"location":"#schedule","text":"The schedule is subject to change. Week 2 Getting started Week 2 Lab 1 Week 4 Lab 2 Week 6 Lab 3 Week 7 Lab 4 Week 9 Lab 5 Week 10 Lab 6 Week 11 Lab 7 Week 12 Lab 8 Week 13 Lab 9","title":"Schedule"},{"location":"get-start/","text":"Getting started The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"get-start/#getting-started","text":"The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"get-start/#installation","text":"Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] . Submission Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Lab 1: Basic Python"},{"location":"lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"lab1/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"lab1/#submission","text":"Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Submission"},{"location":"lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week. Submission Submit the final working code using the Node class to MS Teams.","title":"Lab 2: Breadth-First Search"},{"location":"lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week.","title":"Exercise"},{"location":"lab2/#submission","text":"Submit the final working code using the Node class to MS Teams.","title":"Submission"},{"location":"lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute uniform cost search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly. Submission Submit the working code to MS Teams.","title":"Lab 3: Uniform-Cost Search"},{"location":"lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"lab3/#objective","text":"To create Python script to execute uniform cost search algorithm.","title":"Objective"},{"location":"lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"lab3/#submission","text":"Submit the working code to MS Teams.","title":"Submission"},{"location":"lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/201908/lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Launch the Spyder Python IDE Launch Spyder from Anaconda base environment. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in variable explorer. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named students such that it is a nested array in which contains the name, home country, and home state/province of the students in this class. For example, students = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function with three (3) optional input arguments, name , home_country , home_state . def filterStudent ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter the student based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the students whose detail(s) matches the input. For example, filterStudent(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterStudent(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Lab 1: Basic Python"},{"location":"archive/201908/lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"archive/201908/lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/201908/lab1/#launch-the-spyder-python-ide","text":"Launch Spyder from Anaconda base environment. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in variable explorer.","title":"Launch the Spyder Python IDE"},{"location":"archive/201908/lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/201908/lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/201908/lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/201908/lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/201908/lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/201908/lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/201908/lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/201908/lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/201908/lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/201908/lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/201908/lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/201908/lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/201908/lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/201908/lab1/#exercise","text":"Create a new file in Spyder. Define a variable named students such that it is a nested array in which contains the name, home country, and home state/province of the students in this class. For example, students = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function with three (3) optional input arguments, name , home_country , home_state . def filterStudent ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter the student based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the students whose detail(s) matches the input. For example, filterStudent(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterStudent(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"archive/201908/lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function \\verb|expandAndReturnChildren| to return the children as a list of \\verb|Node| objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it and you will work on that in Lab 3 .","title":"Lab 2: Breadth-First Search"},{"location":"archive/201908/lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/201908/lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/201908/lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"archive/201908/lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"archive/201908/lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"archive/201908/lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"archive/201908/lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"archive/201908/lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"archive/201908/lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"archive/201908/lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function \\verb|expandAndReturnChildren| to return the children as a list of \\verb|Node| objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"archive/201908/lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it and you will work on that in Lab 3 .","title":"Exercise"},{"location":"archive/201908/lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/201908/lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/201908/lab3/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/201908/lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"archive/201908/lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"archive/201908/lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"archive/201908/lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"archive/201908/lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"archive/201908/lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isini ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"archive/201908/lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"archive/201908/lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"archive/201908/lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"archive/201908/lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"archive/201908/lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"archive/201908/lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"archive/201908/lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isini ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"archive/201908/lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/201908/lab6/","text":"Lab 6: Linear Regression Objectives To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library. Dataset Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data? Gradient descent on linear regression model with least squares method Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[ J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[ \\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) \\] \\[ \\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' ) Learning on linear regression model using scikit-learn scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' ) Multivariate linear regression Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[ \\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k \\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by $$ J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ The derivative with respect to \\(\\beta\\) $$ \\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) $$ The derivatives with respect to \\(m_j\\) $$ \\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i) $$ Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Lab 6: Linear Regression"},{"location":"archive/201908/lab6/#lab-6-linear-regression","text":"","title":"Lab 6: Linear Regression"},{"location":"archive/201908/lab6/#objectives","text":"To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library.","title":"Objectives"},{"location":"archive/201908/lab6/#dataset","text":"Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data?","title":"Dataset"},{"location":"archive/201908/lab6/#gradient-descent-on-linear-regression-model-with-least-squares-method","text":"Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[ J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[ \\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i) \\] \\[ \\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' )","title":"Gradient descent on linear regression model with least squares method"},{"location":"archive/201908/lab6/#learning-on-linear-regression-model-using-scikit-learn","text":"scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' )","title":"Learning on linear regression model using scikit-learn"},{"location":"archive/201908/lab6/#multivariate-linear-regression","text":"Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[ \\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k \\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by $$ J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ The derivative with respect to \\(\\beta\\) $$ \\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) $$ The derivatives with respect to \\(m_j\\) $$ \\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i) $$ Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Multivariate linear regression"},{"location":"archive/201908/lab7/","text":"Lab 7: k Nearest Neighbours (KNN) Objective To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset? KNN KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression. Classification Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis. Regression Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Lab 7: *k* Nearest Neighbours (KNN)"},{"location":"archive/201908/lab7/#lab-7-k-nearest-neighbours-knn","text":"","title":"Lab 7: k Nearest Neighbours (KNN)"},{"location":"archive/201908/lab7/#objective","text":"To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression.","title":"Objective"},{"location":"archive/201908/lab7/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/201908/lab7/#knn","text":"KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression.","title":"KNN"},{"location":"archive/201908/lab7/#classification","text":"Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Classification"},{"location":"archive/201908/lab7/#regression","text":"Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Regression"},{"location":"archive/201908/lab8/","text":"Lab 8: Decision Tree Objective To perform CART decision tree learning algorithm using the scikit-learn Python library. Datasets The same iris and diabetes datasets used in Lab 6a are used here. Load the datasets and split them into 80-20 train-test proportion. Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisiontTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation. Regression Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Lab 8: Decision Tree"},{"location":"archive/201908/lab8/#lab-8-decision-tree","text":"","title":"Lab 8: Decision Tree"},{"location":"archive/201908/lab8/#objective","text":"To perform CART decision tree learning algorithm using the scikit-learn Python library.","title":"Objective"},{"location":"archive/201908/lab8/#datasets","text":"The same iris and diabetes datasets used in Lab 6a are used here. Load the datasets and split them into 80-20 train-test proportion.","title":"Datasets"},{"location":"archive/201908/lab8/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/201908/lab8/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/201908/lab8/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisiontTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/201908/lab8/#overfitting","text":"Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/201908/lab8/#regression","text":"Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Regression"},{"location":"archive/201908/lab8/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/201908/lab8/#overfitting_1","text":"Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/201908/lab9/","text":"Lab 9: Clustering Objective To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library. Note Suggestion: use numpy library to simplify the operation. k -means clustering using basic Python The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' ) Dataset The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ]) Initialisation The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 ) Cluster the points The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids ) Update the centroids The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups ) Termination condition The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much. Visualisation Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical? k -means clustering using scikit-learn Python library The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ... Dataset We will use the exact same data generation as the previous section. k-means model The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans ) Training The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... ) Cluster the points The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... ) Visualisation The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Lab 9: Clustering"},{"location":"archive/201908/lab9/#lab-9-clustering","text":"","title":"Lab 9: Clustering"},{"location":"archive/201908/lab9/#objective","text":"To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library.","title":"Objective"},{"location":"archive/201908/lab9/#note","text":"Suggestion: use numpy library to simplify the operation.","title":"Note"},{"location":"archive/201908/lab9/#k-means-clustering-using-basic-python","text":"The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' )","title":"k-means clustering using basic Python"},{"location":"archive/201908/lab9/#dataset","text":"The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ])","title":"Dataset"},{"location":"archive/201908/lab9/#initialisation","text":"The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 )","title":"Initialisation"},{"location":"archive/201908/lab9/#cluster-the-points","text":"The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids )","title":"Cluster the points"},{"location":"archive/201908/lab9/#update-the-centroids","text":"The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups )","title":"Update the centroids"},{"location":"archive/201908/lab9/#termination-condition","text":"The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much.","title":"Termination condition"},{"location":"archive/201908/lab9/#visualisation","text":"Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical?","title":"Visualisation"},{"location":"archive/201908/lab9/#k-means-clustering-using-scikit-learn-python-library","text":"The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ...","title":"k-means clustering using scikit-learn Python library"},{"location":"archive/201908/lab9/#dataset_1","text":"We will use the exact same data generation as the previous section.","title":"Dataset"},{"location":"archive/201908/lab9/#k-means-model","text":"The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans )","title":"k-means model"},{"location":"archive/201908/lab9/#training","text":"The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... )","title":"Training"},{"location":"archive/201908/lab9/#cluster-the-points_1","text":"The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... )","title":"Cluster the points"},{"location":"archive/201908/lab9/#visualisation_1","text":"The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Visualisation"},{"location":"archive/202003/get-start/","text":"Getting started The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"archive/202003/get-start/#getting-started","text":"The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"archive/202003/get-start/#installation","text":"Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"archive/202003/get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"archive/202003/lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] . Submission Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Lab 1: Basic Python"},{"location":"archive/202003/lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"archive/202003/lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/202003/lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/202003/lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/202003/lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/202003/lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/202003/lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/202003/lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/202003/lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/202003/lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/202003/lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/202003/lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/202003/lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/202003/lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/202003/lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/202003/lab1/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"archive/202003/lab1/#submission","text":"Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Submission"},{"location":"archive/202003/lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week. Submission Submit the final working code using the Node class to MS Teams.","title":"Lab 2: Breadth-First Search"},{"location":"archive/202003/lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/202003/lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202003/lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"archive/202003/lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"archive/202003/lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"archive/202003/lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"archive/202003/lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"archive/202003/lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"archive/202003/lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"archive/202003/lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"archive/202003/lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week.","title":"Exercise"},{"location":"archive/202003/lab2/#submission","text":"Submit the final working code using the Node class to MS Teams.","title":"Submission"},{"location":"archive/202003/lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly. Submission Submit the working code to MS Teams.","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202003/lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202003/lab3/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202003/lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"archive/202003/lab3/#submission","text":"Submit the working code to MS Teams.","title":"Submission"},{"location":"archive/202003/lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"archive/202003/lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"archive/202003/lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"archive/202003/lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"archive/202003/lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"archive/202003/lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"archive/202003/lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"archive/202003/lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"archive/202003/lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"archive/202003/lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"archive/202003/lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"archive/202003/lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"archive/202003/lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/202003/lab6/","text":"Lab 6: Linear Regression Objectives To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library. Dataset Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data? Gradient descent on linear regression model with least squares method Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' ) Learning on linear regression model using scikit-learn scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' ) Multivariate linear regression Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Lab 6: Linear Regression"},{"location":"archive/202003/lab6/#lab-6-linear-regression","text":"","title":"Lab 6: Linear Regression"},{"location":"archive/202003/lab6/#objectives","text":"To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library.","title":"Objectives"},{"location":"archive/202003/lab6/#dataset","text":"Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data?","title":"Dataset"},{"location":"archive/202003/lab6/#gradient-descent-on-linear-regression-model-with-least-squares-method","text":"Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' )","title":"Gradient descent on linear regression model with least squares method"},{"location":"archive/202003/lab6/#learning-on-linear-regression-model-using-scikit-learn","text":"scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' )","title":"Learning on linear regression model using scikit-learn"},{"location":"archive/202003/lab6/#multivariate-linear-regression","text":"Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Multivariate linear regression"},{"location":"archive/202003/lab7/","text":"Lab 7: k Nearest Neighbours (KNN) Objective To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset? KNN KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression. Classification Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis. Regression Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Lab 7: *k* Nearest Neighbours (KNN)"},{"location":"archive/202003/lab7/#lab-7-k-nearest-neighbours-knn","text":"","title":"Lab 7: k Nearest Neighbours (KNN)"},{"location":"archive/202003/lab7/#objective","text":"To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression.","title":"Objective"},{"location":"archive/202003/lab7/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/202003/lab7/#knn","text":"KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression.","title":"KNN"},{"location":"archive/202003/lab7/#classification","text":"Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Classification"},{"location":"archive/202003/lab7/#regression","text":"Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Regression"},{"location":"archive/202003/lab8/","text":"Lab 8: Decision Tree Objective To perform CART decision tree learning algorithm using the scikit-learn Python library. Datasets The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion. Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation. Regression Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Lab 8: Decision Tree"},{"location":"archive/202003/lab8/#lab-8-decision-tree","text":"","title":"Lab 8: Decision Tree"},{"location":"archive/202003/lab8/#objective","text":"To perform CART decision tree learning algorithm using the scikit-learn Python library.","title":"Objective"},{"location":"archive/202003/lab8/#datasets","text":"The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion.","title":"Datasets"},{"location":"archive/202003/lab8/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/202003/lab8/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/202003/lab8/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/202003/lab8/#overfitting","text":"Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202003/lab8/#regression","text":"Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Regression"},{"location":"archive/202003/lab8/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/202003/lab8/#overfitting_1","text":"Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202003/lab9/","text":"Lab 9: Clustering Objective To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library. Note Suggestion: use numpy library to simplify the operation. k -means clustering using basic Python The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' ) Dataset The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ]) Initialisation The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 ) Cluster the points The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids ) Update the centroids The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups ) Termination condition The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much. Visualisation Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical? k -means clustering using scikit-learn Python library The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ... Dataset We will use the exact same data generation as the previous section. k-means model The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans ) Training The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... ) Cluster the points The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... ) Visualisation The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Lab 9: Clustering"},{"location":"archive/202003/lab9/#lab-9-clustering","text":"","title":"Lab 9: Clustering"},{"location":"archive/202003/lab9/#objective","text":"To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library.","title":"Objective"},{"location":"archive/202003/lab9/#note","text":"Suggestion: use numpy library to simplify the operation.","title":"Note"},{"location":"archive/202003/lab9/#k-means-clustering-using-basic-python","text":"The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' )","title":"k-means clustering using basic Python"},{"location":"archive/202003/lab9/#dataset","text":"The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ])","title":"Dataset"},{"location":"archive/202003/lab9/#initialisation","text":"The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 )","title":"Initialisation"},{"location":"archive/202003/lab9/#cluster-the-points","text":"The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids )","title":"Cluster the points"},{"location":"archive/202003/lab9/#update-the-centroids","text":"The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups )","title":"Update the centroids"},{"location":"archive/202003/lab9/#termination-condition","text":"The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much.","title":"Termination condition"},{"location":"archive/202003/lab9/#visualisation","text":"Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical?","title":"Visualisation"},{"location":"archive/202003/lab9/#k-means-clustering-using-scikit-learn-python-library","text":"The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ...","title":"k-means clustering using scikit-learn Python library"},{"location":"archive/202003/lab9/#dataset_1","text":"We will use the exact same data generation as the previous section.","title":"Dataset"},{"location":"archive/202003/lab9/#k-means-model","text":"The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans )","title":"k-means model"},{"location":"archive/202003/lab9/#training","text":"The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... )","title":"Training"},{"location":"archive/202003/lab9/#cluster-the-points_1","text":"The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... )","title":"Cluster the points"},{"location":"archive/202003/lab9/#visualisation_1","text":"The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Visualisation"},{"location":"archive/202008/get-start/","text":"Getting started The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"archive/202008/get-start/#getting-started","text":"The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"archive/202008/get-start/#installation","text":"Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"archive/202008/get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"archive/202008/lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] . Submission Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Lab 1: Basic Python"},{"location":"archive/202008/lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"archive/202008/lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/202008/lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/202008/lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/202008/lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/202008/lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/202008/lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/202008/lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/202008/lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/202008/lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/202008/lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/202008/lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 != Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/202008/lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/202008/lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/202008/lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/202008/lab1/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"archive/202008/lab1/#submission","text":"Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Submission"},{"location":"archive/202008/lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week. Submission Submit the final working code using the Node class to MS Teams.","title":"Lab 2: Breadth-First Search"},{"location":"archive/202008/lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/202008/lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202008/lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"archive/202008/lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"archive/202008/lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"archive/202008/lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"archive/202008/lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"archive/202008/lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main_' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"archive/202008/lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"archive/202008/lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"archive/202008/lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week.","title":"Exercise"},{"location":"archive/202008/lab2/#submission","text":"Submit the final working code using the Node class to MS Teams.","title":"Submission"},{"location":"archive/202008/lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly. Submission Submit the working code to MS Teams.","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202008/lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202008/lab3/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202008/lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"archive/202008/lab3/#submission","text":"Submit the working code to MS Teams.","title":"Submission"},{"location":"archive/202008/lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"archive/202008/lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"archive/202008/lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"archive/202008/lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"archive/202008/lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"archive/202008/lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"archive/202008/lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"archive/202008/lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"archive/202008/lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"archive/202008/lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"archive/202008/lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"archive/202008/lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"archive/202008/lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/202008/lab6/","text":"Lab 6: Linear Regression Objectives To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library. Dataset Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data? Gradient descent on linear regression model with least squares method Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' ) Learning on linear regression model using scikit-learn scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' ) Multivariate linear regression Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Lab 6: Linear Regression"},{"location":"archive/202008/lab6/#lab-6-linear-regression","text":"","title":"Lab 6: Linear Regression"},{"location":"archive/202008/lab6/#objectives","text":"To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library.","title":"Objectives"},{"location":"archive/202008/lab6/#dataset","text":"Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data?","title":"Dataset"},{"location":"archive/202008/lab6/#gradient-descent-on-linear-regression-model-with-least-squares-method","text":"Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' )","title":"Gradient descent on linear regression model with least squares method"},{"location":"archive/202008/lab6/#learning-on-linear-regression-model-using-scikit-learn","text":"scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' )","title":"Learning on linear regression model using scikit-learn"},{"location":"archive/202008/lab6/#multivariate-linear-regression","text":"Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Multivariate linear regression"},{"location":"archive/202008/lab7/","text":"Lab 7: k Nearest Neighbours (KNN) Objective To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset? KNN KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression. Classification Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis. Regression Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Lab 7: *k* Nearest Neighbours (KNN)"},{"location":"archive/202008/lab7/#lab-7-k-nearest-neighbours-knn","text":"","title":"Lab 7: k Nearest Neighbours (KNN)"},{"location":"archive/202008/lab7/#objective","text":"To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression.","title":"Objective"},{"location":"archive/202008/lab7/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/202008/lab7/#knn","text":"KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression.","title":"KNN"},{"location":"archive/202008/lab7/#classification","text":"Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Classification"},{"location":"archive/202008/lab7/#regression","text":"Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Regression"},{"location":"archive/202008/lab8/","text":"Lab 8: Decision Tree Objective To perform CART decision tree learning algorithm using the scikit-learn Python library. Datasets The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion. Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation. Regression Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Lab 8: Decision Tree"},{"location":"archive/202008/lab8/#lab-8-decision-tree","text":"","title":"Lab 8: Decision Tree"},{"location":"archive/202008/lab8/#objective","text":"To perform CART decision tree learning algorithm using the scikit-learn Python library.","title":"Objective"},{"location":"archive/202008/lab8/#datasets","text":"The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion.","title":"Datasets"},{"location":"archive/202008/lab8/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/202008/lab8/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/202008/lab8/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/202008/lab8/#overfitting","text":"Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202008/lab8/#regression","text":"Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Regression"},{"location":"archive/202008/lab8/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/202008/lab8/#overfitting_1","text":"Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202008/lab9/","text":"Lab 9: Clustering Objective To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library. Note Suggestion: use numpy library to simplify the operation. k -means clustering using basic Python The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' ) Dataset The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ]) Initialisation The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 ) Cluster the points The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids ) Update the centroids The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups ) Termination condition The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much. Visualisation Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical? k -means clustering using scikit-learn Python library The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ... Dataset We will use the exact same data generation as the previous section. k-means model The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans ) Training The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... ) Cluster the points The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... ) Visualisation The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Lab 9: Clustering"},{"location":"archive/202008/lab9/#lab-9-clustering","text":"","title":"Lab 9: Clustering"},{"location":"archive/202008/lab9/#objective","text":"To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library.","title":"Objective"},{"location":"archive/202008/lab9/#note","text":"Suggestion: use numpy library to simplify the operation.","title":"Note"},{"location":"archive/202008/lab9/#k-means-clustering-using-basic-python","text":"The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' )","title":"k-means clustering using basic Python"},{"location":"archive/202008/lab9/#dataset","text":"The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ])","title":"Dataset"},{"location":"archive/202008/lab9/#initialisation","text":"The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 )","title":"Initialisation"},{"location":"archive/202008/lab9/#cluster-the-points","text":"The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids )","title":"Cluster the points"},{"location":"archive/202008/lab9/#update-the-centroids","text":"The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups )","title":"Update the centroids"},{"location":"archive/202008/lab9/#termination-condition","text":"The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much.","title":"Termination condition"},{"location":"archive/202008/lab9/#visualisation","text":"Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical?","title":"Visualisation"},{"location":"archive/202008/lab9/#k-means-clustering-using-scikit-learn-python-library","text":"The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ...","title":"k-means clustering using scikit-learn Python library"},{"location":"archive/202008/lab9/#dataset_1","text":"We will use the exact same data generation as the previous section.","title":"Dataset"},{"location":"archive/202008/lab9/#k-means-model","text":"The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans )","title":"k-means model"},{"location":"archive/202008/lab9/#training","text":"The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... )","title":"Training"},{"location":"archive/202008/lab9/#cluster-the-points_1","text":"The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... )","title":"Cluster the points"},{"location":"archive/202008/lab9/#visualisation_1","text":"The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Visualisation"},{"location":"archive/202103/get-start/","text":"Getting started The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"archive/202103/get-start/#getting-started","text":"The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"archive/202103/get-start/#installation","text":"Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"archive/202103/get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"archive/202103/lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] . Submission Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Lab 1: Basic Python"},{"location":"archive/202103/lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"archive/202103/lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/202103/lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/202103/lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/202103/lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/202103/lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/202103/lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/202103/lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/202103/lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/202103/lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/202103/lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/202103/lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/202103/lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/202103/lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/202103/lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/202103/lab1/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"archive/202103/lab1/#submission","text":"Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Submission"},{"location":"archive/202103/lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week. Submission Submit the final working code using the Node class to MS Teams.","title":"Lab 2: Breadth-First Search"},{"location":"archive/202103/lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/202103/lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202103/lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"archive/202103/lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"archive/202103/lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"archive/202103/lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"archive/202103/lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"archive/202103/lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"archive/202103/lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"archive/202103/lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"archive/202103/lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week.","title":"Exercise"},{"location":"archive/202103/lab2/#submission","text":"Submit the final working code using the Node class to MS Teams.","title":"Submission"},{"location":"archive/202103/lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly. Submission Submit the working code to MS Teams.","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202103/lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202103/lab3/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202103/lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"archive/202103/lab3/#submission","text":"Submit the working code to MS Teams.","title":"Submission"},{"location":"archive/202103/lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"archive/202103/lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"archive/202103/lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"archive/202103/lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"archive/202103/lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"archive/202103/lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"archive/202103/lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"archive/202103/lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"archive/202103/lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"archive/202103/lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"archive/202103/lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"archive/202103/lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"archive/202103/lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/202103/lab6/","text":"Lab 6: Linear Regression Objectives To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library. Dataset Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data? Gradient descent on linear regression model with least squares method Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' ) Learning on linear regression model using scikit-learn scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' ) Multivariate linear regression Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Lab 6: Linear Regression"},{"location":"archive/202103/lab6/#lab-6-linear-regression","text":"","title":"Lab 6: Linear Regression"},{"location":"archive/202103/lab6/#objectives","text":"To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library.","title":"Objectives"},{"location":"archive/202103/lab6/#dataset","text":"Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data?","title":"Dataset"},{"location":"archive/202103/lab6/#gradient-descent-on-linear-regression-model-with-least-squares-method","text":"Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' )","title":"Gradient descent on linear regression model with least squares method"},{"location":"archive/202103/lab6/#learning-on-linear-regression-model-using-scikit-learn","text":"scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' )","title":"Learning on linear regression model using scikit-learn"},{"location":"archive/202103/lab6/#multivariate-linear-regression","text":"Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Multivariate linear regression"},{"location":"archive/202103/lab7/","text":"Lab 7: k Nearest Neighbours (KNN) Objective To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset? KNN KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression. Classification Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis. Regression Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Lab 7: *k* Nearest Neighbours (KNN)"},{"location":"archive/202103/lab7/#lab-7-k-nearest-neighbours-knn","text":"","title":"Lab 7: k Nearest Neighbours (KNN)"},{"location":"archive/202103/lab7/#objective","text":"To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression.","title":"Objective"},{"location":"archive/202103/lab7/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/202103/lab7/#knn","text":"KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression.","title":"KNN"},{"location":"archive/202103/lab7/#classification","text":"Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. ```python import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Classification"},{"location":"archive/202103/lab7/#regression","text":"Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Regression"},{"location":"archive/202103/lab8/","text":"Lab 8: Decision Tree Objective To perform CART decision tree learning algorithm using the scikit-learn Python library. Datasets The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion. Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation. Regression Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Lab 8: Decision Tree"},{"location":"archive/202103/lab8/#lab-8-decision-tree","text":"","title":"Lab 8: Decision Tree"},{"location":"archive/202103/lab8/#objective","text":"To perform CART decision tree learning algorithm using the scikit-learn Python library.","title":"Objective"},{"location":"archive/202103/lab8/#datasets","text":"The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion.","title":"Datasets"},{"location":"archive/202103/lab8/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/202103/lab8/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/202103/lab8/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/202103/lab8/#overfitting","text":"Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202103/lab8/#regression","text":"Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Regression"},{"location":"archive/202103/lab8/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/202103/lab8/#overfitting_1","text":"Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202103/lab9/","text":"Lab 9: Clustering Objective To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library. Note Suggestion: use numpy library to simplify the operation. k -means clustering using basic Python The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' ) Dataset The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ]) Initialisation The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 ) Cluster the points The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids ) Update the centroids The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups ) Termination condition The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much. Visualisation Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical? k -means clustering using scikit-learn Python library The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ... Dataset We will use the exact same data generation as the previous section. k-means model The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans ) Training The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... ) Cluster the points The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... ) Visualisation The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Lab 9: Clustering"},{"location":"archive/202103/lab9/#lab-9-clustering","text":"","title":"Lab 9: Clustering"},{"location":"archive/202103/lab9/#objective","text":"To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library.","title":"Objective"},{"location":"archive/202103/lab9/#note","text":"Suggestion: use numpy library to simplify the operation.","title":"Note"},{"location":"archive/202103/lab9/#k-means-clustering-using-basic-python","text":"The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' )","title":"k-means clustering using basic Python"},{"location":"archive/202103/lab9/#dataset","text":"The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ])","title":"Dataset"},{"location":"archive/202103/lab9/#initialisation","text":"The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 )","title":"Initialisation"},{"location":"archive/202103/lab9/#cluster-the-points","text":"The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids )","title":"Cluster the points"},{"location":"archive/202103/lab9/#update-the-centroids","text":"The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups )","title":"Update the centroids"},{"location":"archive/202103/lab9/#termination-condition","text":"The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much.","title":"Termination condition"},{"location":"archive/202103/lab9/#visualisation","text":"Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical?","title":"Visualisation"},{"location":"archive/202103/lab9/#k-means-clustering-using-scikit-learn-python-library","text":"The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ...","title":"k-means clustering using scikit-learn Python library"},{"location":"archive/202103/lab9/#dataset_1","text":"We will use the exact same data generation as the previous section.","title":"Dataset"},{"location":"archive/202103/lab9/#k-means-model","text":"The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans )","title":"k-means model"},{"location":"archive/202103/lab9/#training","text":"The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... )","title":"Training"},{"location":"archive/202103/lab9/#cluster-the-points_1","text":"The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... )","title":"Cluster the points"},{"location":"archive/202103/lab9/#visualisation_1","text":"The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Visualisation"},{"location":"archive/202203/get-start/","text":"Getting started The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments. Installation Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda. Launching IDE You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Getting started"},{"location":"archive/202203/get-start/#getting-started","text":"The lab for CSC3206 Artificial Intelligence will be using Python as the programming language. The Anaconda distribution of Python is recommended, however, if you are familiar and comfortable with the vanilla distribution of Python. For those who have not installed Python in their machine, proceed to next session . This page only covers the installation of the Anaconda platform as, in my opinion, it is more beginner friendly in terms of installing packages and encapsulating environments.","title":"Getting started"},{"location":"archive/202203/get-start/#installation","text":"Download the Anaconda installer with Python 3.7 for your system from https://www.anaconda.com/distribution/ . Use the graphical installer to install Anaconda.","title":"Installation"},{"location":"archive/202203/get-start/#launching-ide","text":"You will be using the Spyder IDE for Python. Feel free to use other IDE or code editor with terminal if that's your preference. Start the Anaconda Navigator from your application list. From Anaconda base environment, launch Spyder IDE. The Spyder IDE consists of three parts: the editor, the variable explorer, and the IPython Console. The editor is where you write your codes. The variable explorer shows the value of the variable after running the code. The IPython console allows you to execute commands, interact with the running code, and display visualisation. After the code is written in the editor, you can execute the code using F5 to run file. Then the variables and their values can be found in the variable explorer.","title":"Launching IDE"},{"location":"archive/202203/lab1/","text":"Lab 1: Basic Python Objective To understand basic syntax of Python programming language. Declare a variable Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1 Array manipulation Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order Add an element to the end of an array # in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4] Add multiple elements to the end of an array # in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Assign a value to a specific index # in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Insert an element at a specific index # in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Remove an element at a specific index # in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e'] Combining arrays zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')] Loops Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n )) Conditions The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or User-defined functions To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message User-defined functions with input arguments To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message User-defined functions with optional input arguments To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message Exercise Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] . Submission Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Lab 1: Basic Python"},{"location":"archive/202203/lab1/#lab-1-basic-python","text":"","title":"Lab 1: Basic Python"},{"location":"archive/202203/lab1/#objective","text":"To understand basic syntax of Python programming language.","title":"Objective"},{"location":"archive/202203/lab1/#declare-a-variable","text":"Python is strongly and dynamically typed. Strongly typed means the type of a variable does not change unexpectedly. When a variable is defined as a string with only numerical digits, it stays string, it doesn\u2019t become an integer or number. Dynamically typed means the type of a variable can change when a value of a different type is assigned to the variable. As Python is dynamically typed, we do not need to specify a type when we declare a variable. We just assign a value to the variable. Define a variable named val and assign the value 'a' to the variable. val = 'a' The type of the variable can be viewed in the variable explorer. Continue from the previous code block, assign the value 1 to the same variable. The variable will now be of type int instead of type str . val = 1","title":"Declare a variable"},{"location":"archive/202203/lab1/#array-manipulation","text":"Array in Python can be declared using a set of square brackets ( [...] ). To assign a variable with an empty array, arr = [] To define an array with a series of numbers, arr = [ 1 , 2 , 3 , 4 , 5 ] To define an array with a series of alphabets, arr = [ 'a' , 'b' , 'c' , 'd' , 'e' ] An array can also be defined with values of mixed types. arr = [ 1 , 'a' , 2 , 'b' , 3 , 'c' ] Python arrays (or more commonly known as lists) are zero indexed arrays; it means to access the first element in the array arr , # in IPython console arr [ 0 ] # gives the output of 1 arr [ 1 ] # gives the output of 'a' Python arrays also support negative indexing; this means to get the last element in the array arr , # in IPython console arr [ - 1 ] # gives the output of 'c' Colon ( : ) can be used to extract multiple elements from an array. Maximum two (2) colons can be used for indexing/slicing an array. arr[0:5:2] The value before the first colon is the starting index, the value after the first colon is the ending index (exclusive), the value after the second colon is the number of steps. If the first value is empty, it is assumed as 0 . If the second value is empty, it is assumed as the length of the array, i.e. up till the last element in the array. If the third value is empty, it is assumed as 1 . # in IPython console arr # [1, 'a', 2, 'b', 3, 'c'] arr [ 1 : 5 ] # ['a', 2, 'b', 3] arr [ 1 : 5 : 2 ] # ['a', 'b'] arr [: 3 ] # [1, 'a', 2] arr [ 4 :] # [3, 'c'] arr [ 2 : - 2 ] # [2, 'b'] arr [ 4 : 1 ] # [] arr [ 4 : 1 : - 1 ] # [3, 'b', 2] --> slice in the reverse order","title":"Array manipulation"},{"location":"archive/202203/lab1/#add-an-element-to-the-end-of-an-array","text":"# in IPython console arr . append ( 4 ) arr # [1, 'a', 2, 'b', 3, 'c', 4]","title":"Add an element to the end of an array"},{"location":"archive/202203/lab1/#add-multiple-elements-to-the-end-of-an-array","text":"# in IPython console arr . extend ([ 'd' , 5 , 'e' ]) arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Add multiple elements to the end of an array"},{"location":"archive/202203/lab1/#assign-a-value-to-a-specific-index","text":"# in IPython console arr [ 0 ] = 0 arr # [0, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Assign a value to a specific index"},{"location":"archive/202203/lab1/#insert-an-element-at-a-specific-index","text":"# in IPython console arr . insert ( 1 , 1 ) arr # [0, 1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Insert an element at a specific index"},{"location":"archive/202203/lab1/#remove-an-element-at-a-specific-index","text":"# in IPython console del arr [ 0 ] arr # [1, 'a', 2, 'b', 3, 'c', 4, 'd', 5, 'e']","title":"Remove an element at a specific index"},{"location":"archive/202203/lab1/#combining-arrays","text":"zip can be used to combine two or more arrays. # in editor joint_arr = list ( zip ([ 1 , 2 , 3 ], [ 'a' , 'b' , 'c' ])) # in IPython console joint_arr # [(1,'a'),(2,'b'),(3,'c')]","title":"Combining arrays"},{"location":"archive/202203/lab1/#loops","text":"Python supports for and while loops. To loop through every element in the array arr and print them to the console, for a in arr : print ( a ) # 1 # a # 2 # ... for a in arr : loops through all elements in arr and in each loop, an element in arr is assigned to the variable a . Note that in most other programming languages, code blocks are separated with delimiters such as the curly brackets ( {} ). This is not the case in Python. Code blocks in Python are defined by their indentation and normally initiated with a colon( : ). For example, for a in arr : print ( a ) print ( arr ) print(a) is the command to be executed in each loop. print(arr) is only executed after the for loop is completed. for a in arr : print ( a ) print ( arr ) In this case, print(arr) is executed in each loop. Using zip we can loop through two arrays at once. for item in zip ([ 'a' , 'b' , 'c' , 'd' ],[ 'artificial' , 'breadth' , 'cost' , 'depth' ]): print ( item [ 0 ] + ' for ' + item [ 1 ]) enumerate is useful in obtaining the index of the element in the array. for ( index , item ) in enumerate ([ 'a' , 'b' , 'c' , 'd' ]): print ( 'Index of ' + item + ' is: ' + str ( index )) Looping through a dictionary ( dict ) can also be done easily. x_dict = { 'd' : 'depth' , 'e' : 'estimation' , 'f' : 'frontier' } for key , value in x_dict . items (): print ( key + ' for ' + value ) while loops can be defined similarly. x = 0 while x != 10 : x += 1 print ( x ) print ( 'while loop is completed' ) The following example introduces nested array and multiple assignments. arr = [[ 'a' , 1 ],[ 'b' , 2 ],[ 'c' , 3 ],[ 'd' , 4 ],[ 'e' , 5 ],[ 'f' , 6 ]] for [ a , n ] in arr : print ( str ( a ) + ' is ' + str ( n ))","title":"Loops"},{"location":"archive/202203/lab1/#conditions","text":"The following operators can be used for conditional testing: Operator Definition == Equivalence != Inequivalence < Less than <= Less than or equal to > Greater than >= Greater than or equal to Python also supports text operator for conditional testing: Operator Definition Example (symbolic) Example (text) is Equivalence a == 1 a is 1 not Inequivalence a != 1 a is not 1 or not (a is 1) or not a is 1 or not(a == 1) Combining two conditions can also be done with text operators and and or . Symbolic operator Text operator & and | or","title":"Conditions"},{"location":"archive/202203/lab1/#user-defined-functions","text":"To define a custom function with the name of custom_fcn , def custom_fcn (): print ( 'This is a custom function to display custom message' ) To call the function, custom_fcn () # This is a custom function to display custom message","title":"User-defined functions"},{"location":"archive/202203/lab1/#user-defined-functions-with-input-arguments","text":"To define a custom function with input arguments, def custom_fcn ( msg ): print ( 'This is a custom function to display ' + msg ) The function can be called by custom_fcn ( 'new message' ) # This is a custom function to display new message","title":"User-defined functions with input arguments"},{"location":"archive/202203/lab1/#user-defined-functions-with-optional-input-arguments","text":"To define a custom function with optional input arguments, we just need to provide the default value to the optional input arguments. def custom_fcn ( msg = 'default message' ): print ( 'This is a custom function to display ' + msg ) The input arguments can also be specified as named inputs. custom_fcn ( msg = 'new message' ) # This is a custom function to display new message","title":"User-defined functions with optional input arguments"},{"location":"archive/202203/lab1/#exercise","text":"Create a new file in Spyder. Define a variable named friends such that it is a nested array in which contains the name, home country, and home state/province of 10 of your friends (real or virtual). For example, friends = [[ \"James\" , \"Malaysia\" , \"Malacca\" ], [ \"Goh\" , \"Australia\" , \"Brisbane\" ], [ \"Don\" , \"Malaysia\" , \"Pahang\" ]] Create a function in the same file with three (3) optional input arguments, name , home_country , home_state . def filterFriend ( name = \"\" , home_country = \"\" , home_state = \"\" ): ... return filtered This function will filter friends based on the input arguments provided. The function will ignore the input argument if it has empty string, i.e. \"\" . If any of the input arguments is provided, the function will find the friends whose detail(s) matches the input. For example, filterFriend(name=\"James\") will return [[\"James\", \"Malaysia\", \"Malacca\"]] filterFriend(home_country=\"Malaysia\") will return [[\"James\", \"Malaysia\", \"Malacca\"], [\"Don\", \"Malaysia\", \"Pahang\"]] .","title":"Exercise"},{"location":"archive/202203/lab1/#submission","text":"Save the file you created in Exercise using your student id as the file name, for example, 12345678.py . Submit this file on MS Teams.","title":"Submission"},{"location":"archive/202203/lab2/","text":"Lab 2: Breadth-First Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Translating the state space First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest' Actions and transition model Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem. Breadth-first search algorithm Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child ) Oops, something is not right Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier. Running the algorithm Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error. Redundant storing of states? When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier . Using a class The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost ) Exercise Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week. Submission Submit the final working code using the Node class to MS Teams.","title":"Lab 2: Breadth-First Search"},{"location":"archive/202203/lab2/#lab-2-breadth-first-search","text":"","title":"Lab 2: Breadth-First Search"},{"location":"archive/202203/lab2/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202203/lab2/#problem-to-be-solved","text":"The search problem we will focus on during this lab is Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest. The road map of Romania, which is the state space of the problem is given as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } });","title":"Problem to be solved"},{"location":"archive/202203/lab2/#translating-the-state-space","text":"First we need to define the state space in our problem. The important information from the state space is the connections between states and the step costs between connected states. Notice that in this problem the connections are reversible. Therefore in our state space the connection from Arad to Zerind and the connection from Zerind to Arad are identical, hence only one instance of that connection is needed. The most straightforward way of defining the state space is by using a nested array, in which each inner array consists of the two connected states and its cost. Open a script file and define the variable state_space . The following code shows the first three elements in the nested array. Complete the definition of the variable by referring to the state space provided. state_space = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Timisoara' , 'Lugoj' , 111 ], ... ] Define two variables initial_state and goal_state to hold the information from the problem. initial_state = 'Arad' goal_state = 'Bucharest'","title":"Translating the state space"},{"location":"archive/202203/lab2/#actions-and-transition-model","text":"Actions and transition model provide us the way to identify the children of a node in a search tree. In this problem, the actions and transition model are straightforward. The actions are limited by the states connected to node and the transition model is directly defined by the action. Therefore we can create a function to search through the state space to find the children of a node, which would be suffice as actions and transition model. To achieve this, we will loop through the state_space to check for any connection linked to the node, the other state on the connection will be the child of the node in the search tree. In the same script file, define the following function: def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node : children . append ( n ) elif n == node : children . append ( m ) return children This function provides the children of a node given the state_space of the problem.","title":"Actions and transition model"},{"location":"archive/202203/lab2/#breadth-first-search-algorithm","text":"Next we will create a function to execute the search algorithm. The first algorithm we will use is the breadth-first search (BFS). As we want to separate the problem definition from the algorithm definition, the algorithm function will have the inputs of the state space, initial state and goal state. def bfs ( state_space , initial_state , goal_state ): In this function, we need two empty arrays to store the frontier and the explored states. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] At initial stage, the initial state is our frontier. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] frontier . append ( initial_state ) When we are generating the search tree, we will continually expanding the first node (FIFO) in the frontier until we generate a node with goal state. Therefore we need to have a loop to repeatedly expanding the first node in the frontier until the goal node is generated. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False frontier . append ( initial_state ) while not found_goal : ... In the loop we will first expand the first node in the frontier to obtain the children, and move the expanded node from frontier to the explored set. while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ]) # remove the expanded node from the frontier del frontier [ 0 ] Check if the children generated should be added to the frontier or discarded. If any child is expanded previously, i.e. in the explored set, or if it is generated previously but not expanded yet, i.e. in the frontier, then it should be discarded. Else, it should be added to the frontier. del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child in explored ) and not ( child in frontier ): # add child to the frontier frontier . append ( child ) Before a child is added to the frontier, it should be tested for goal. If it has the goal state, the BFS algorithm should in fact be terminated and return the solution. if not ( child in explored ) and not ( child in frontier ): # goal test if child == goal_state : found_goal = True break # add child to the frontiers frontier . append ( child )","title":"Breadth-first search algorithm"},{"location":"archive/202203/lab2/#oops-something-is-not-right","text":"Now, if you are following, you may notice that we have a way to end the algorithm but we have failed to store our solution. One way to store the solution while generating the search tree is to store the frontier as the paths from the initial state to the leaf nodes instead of just the leaf nodes. Therefore we would be able to retain the memory of the explored section of the search tree. First we need to modify the expandAndReturnChildren function. def expandAndReturnChildren ( state_space , path_to_leaf_node ): children = [] for [ m , n , c ] in state_space : if m == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ n ]) elif n == path_to_leaf_node [ - 1 ]: children . append ( path_to_leaf_node + [ m ]) return children 4. The bfs function is also modified to accommodate to this change. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False solution = [] frontier . append ([ initial_state ]) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # copy the node to the explored set explored . append ( frontier [ 0 ][ - 1 ]) # remove the expanded frontier del frontier [ 0 ] # loop through the children for child in children : # check if a node was expanded or generated previously if not ( child [ - 1 ] in explored ) and not ( child [ - 1 ] in [ f [ - 1 ] for f in frontier ]): # goal test if child [ - 1 ] == goal_state : found_goal = True solution = child # add children to the frontier frontier . append ( child ) print ( \"Explored: \" ) print ( explored ) print ( \"Frontier:\" ) for f in frontier : print ( f ) print ( \"Children: \" ) print ( children ) print ( \"\" ) return solution Noted that in line 12 we are only saving the leaf node that has just been expanded to the explored set since the information of the path is unnecessary to be stored in the explored set. In line 18 , [f[-1] for f in frontier] is used to obtain a list of the leaf nodes in the frontier. This is the pythonic way of extracting from a list to form another list. The one-liner is equivalent to leaf_nodes = [] for f in frontier : leaf_nodes . append ( f [ - 1 ]) f[-1] is used since currently we are saving the path from initial state to the respective leaf node in the frontier.","title":"Oops, something is not right"},{"location":"archive/202203/lab2/#running-the-algorithm","text":"Now we have two functions expandAndReturnChildren and bfs , alongside with the variables state_space , initial_state , and goal_state . To run a script to execute the bfs function, we can have the script file structured as such: def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) Beware that in Python, a .py file can also be used to define a Python library/module. To prevent the commands outside the function to be executed when the file is used as a library instead of a script, we can implement an extra condition check. def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' print ( 'Solution: ' + str ( bfs ( state_space , initial_state , goal_state ))) __name__ is a special variable in Python that evaluates to the name of the current module. This variable has the value of '__main__' if it's called as the main program rather than a module or library. This part is essentially the main function in other programming languages like C++ and Java. Compile the program and resolve any error.","title":"Running the algorithm"},{"location":"archive/202203/lab2/#redundant-storing-of-states","text":"When you run the script from previous section, you may wonder, if there is a more efficient way of memory usage instead of saving the path to each leaf node in the frontier. Currently there is a lot of redundant states being stored in the variable frontier .","title":"Redundant storing of states?"},{"location":"archive/202203/lab2/#using-a-class","text":"The alternative and more space-efficient way would be to declare each node in the search tree to be an object that stores its name, parent, and children. To do so, we need to first define a class. class Node : def __init__ ( self , state = None , parent = None ): self . state = state self . parent = parent self . children = [] def addChildren ( self , children ): self . children . extend ( children ) In Python, to define a class, the class needs to have the function __init__ with at least one input self . This function is called when an object is initiated with this class. The internal variable self defines the properties of the object. The input arguments apart from self for the function __init__ are the parameters to be passed when initiating a new object. In a class, additional functions can also be defined, and these will be the methods for the object of this class. An example of initiating a new object for this class and use the function is: a_distinct_node = Node ( 'state name' , 'parent of this node' ) a_distinct_node . addChildren ([ 'child 1' , 'child 2' ]) With the new class, we can update the function expandAndReturnChildren to return the children as a list of Node objects. def expandAndReturnChildren ( state_space , node ): children = [] for [ m , n , c ] in state_space : if m == node . state : children . append ( Node ( n , node . state )) elif n == node . state : children . append ( Node ( m , node . state )) return children With the availability of the Node class, we can save the nodes as Node objects in the frontier instead of paths to the leaf nodes. When the goal state is found, the goal node is used to trace back to its predecessor (a.k.a. parent, which will be now in the explored set) which is accessible using the property .parent of the goal node. Then the predecessor to the parent of the goal node can be traced similarly. This process is thus repeated until the initial state (with no parent) to obtain the solution and its cost. def bfs ( state_space , initial_state , goal_state ): frontier = [] explored = [] found_goal = False goalie = Node () solution = [] # add initial state to frontier frontier . append ( Node ( initial_state , None )) while not found_goal : # expand the first in the frontier children = expandAndReturnChildren ( state_space , frontier [ 0 ]) # add children list to the expanded node frontier [ 0 ] . addChildren ( children ) # add to the explored list explored . append ( frontier [ 0 ]) # remove the expanded frontier del frontier [ 0 ] # add children to the frontier for child in children : # check if a node was expanded or generated previously if not ( child . state in [ e . state for e in explored ]) and not ( child . state in [ f . state for f in frontier ]): # goal test if child . state == goal_state : found_goal = True goalie = child frontier . append ( child ) print ( \"Explored:\" , [ e . state for e in explored ]) print ( \"Frontier:\" , [ f . state for f in frontier ]) print ( \"Children:\" , [ c . state for c in children ]) print ( \"\" ) solution = [ goalie . state ] path_cost = 0 while goalie . parent is not None : solution . insert ( 0 , goalie . parent ) for e in explored : if e . state == goalie . parent : path_cost += getCost ( state_space , e . state , goalie . state ) goalie = e break return solution , path_cost def getCost ( state_space , state0 , state1 ): for [ m , n , c ] in state_space : if [ state0 , state1 ] == [ m , n ] or [ state1 , state0 ] == [ m , n ]: return c The compiled program will be structured as such. Compile and run the program. class Node : ... def expandAndReturnChildren ( ... ): ... def bfs ( ... ): ... def getCost ( ... ): ... if __name__ == '__main__' : state_space = [ ... ] initial_state = 'Arad' goal_state = 'Bucharest' [ solution , cost ] = bfs ( state_space , initial_state , goal_state ) print ( \"Solution:\" , solution ) print ( \"Path Cost:\" , cost )","title":"Using a class"},{"location":"archive/202203/lab2/#exercise","text":"Fully understand the code as you will have to modify the code for other problem/search algorithms in next labs. How would you modify the code to run other uninformed search algorithms such as uniform-cost search, depth-first search, etc.? Which part(s) of the code do you need to modify? Think about it before you work on that in Lab 3 next week.","title":"Exercise"},{"location":"archive/202203/lab2/#submission","text":"Submit the final working code using the Node class to MS Teams.","title":"Submission"},{"location":"archive/202203/lab3/","text":"Lab 3: Uniform-Cost Search Objective To create Python script to execute breadth first search algorithm. Problem to be solved We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly. Submission Submit the working code to MS Teams.","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202203/lab3/#lab-3-uniform-cost-search","text":"","title":"Lab 3: Uniform-Cost Search"},{"location":"archive/202203/lab3/#objective","text":"To create Python script to execute breadth first search algorithm.","title":"Objective"},{"location":"archive/202203/lab3/#problem-to-be-solved","text":"We will revisit Nick\u2019s route-finding problem in Romania, starting in Arad to reach Bucharest, and implement uniform-cost search to solve the problem. 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\", data: { circleradius: 10 } }); Uniform cost search changes the sorting of the frontier by ordering it with its path cost up to the leaf node and expanding the leaf node with the lowest cost. Based on the code from previous lab, add the code to sort the frontier following the path cost up to the leaf node. If a latter-found node has the same state as a previous node and the previous node has been expanded, what should be done? What happens when a latter-found node has the same state as a previous node and have a shorter path cost? How do we implement this in our code? Note also, the goal test should be applied during expansion, not generation. Execute the program and investigate if the program is working correctly.","title":"Problem to be solved"},{"location":"archive/202203/lab3/#submission","text":"Submit the working code to MS Teams.","title":"Submission"},{"location":"archive/202203/lab4/","text":"Lab 4: State representation Objective To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search. Problem The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Lab 4: State representation"},{"location":"archive/202203/lab4/#lab-4-state-representation","text":"","title":"Lab 4: State representation"},{"location":"archive/202203/lab4/#objective","text":"To represent the state of a vacuum world. To solve the vacuum world problem using breadth-first search.","title":"Objective"},{"location":"archive/202203/lab4/#problem","text":"The following image represents one of the possible states of a two-square vacuum world. The aim of this section is to create a code to search for solution for the vacuum world given an initial state. Consider the actions left , right , suck , and every action contributes the same cost. How could we represent the state? Do we need to define the whole state space statically? How do we define the transition model? The transition model should be a function that takes the state and action as inputs and returns the result state . Create the code to solve a vacuum world problem using the breadth-first search. Execute it and investigate if it's working correctly.","title":"Problem"},{"location":"archive/202203/lab5/","text":"Lab 5: pandas Objective To learn the basic of the pandas Python library. Data structures In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column Instruction For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes. Imports To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names. Series Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s DataFrame Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])] Data manipulation pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Lab 5: pandas"},{"location":"archive/202203/lab5/#lab-5-pandas","text":"","title":"Lab 5: pandas"},{"location":"archive/202203/lab5/#objective","text":"To learn the basic of the pandas Python library.","title":"Objective"},{"location":"archive/202203/lab5/#data-structures","text":"In pandas, there are two types of data structures: Structure Description Series 1D labeled homogeneously-typed array DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column","title":"Data structures"},{"location":"archive/202203/lab5/#instruction","text":"For all sections in this lab other than the last section, use the IPython console (located normally at the right bottom corner) to run the codes.","title":"Instruction"},{"location":"archive/202203/lab5/#imports","text":"To import the pandas library, import pandas as pd NumPy is a dependency of pandas and also a powerful Python library for scientific data processing. We may need to use NumPy from time to time. To import Numpy , import numpy as np It's common practice to import pandas as pd and numpy as np . You would see this a lot if you tried to search for tutorials or solutions online. However, it's just a convention, it is fine to use other names.","title":"Imports"},{"location":"archive/202203/lab5/#series","text":"Creation from list, s1 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ]) s2 = pd . Series ([ 1 , 3 , 5 , np . nan , 6 , 8 ], index = [ 1 , 2 , 3 , 4 , 5 , 'f' ]) What is the difference between s1 and s2 ? from dict, d = { 'a' : 1 , 'b' : 2 , 'c' : 3 } s3 = pd . Series ( d ) from scalar value, s4 = pd . Series ( 5 , index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) Indexing of Series try the following code to understand the getting and setting of a series with default indexing. s = pd . Series ( np . random . randn ( 5 )) s [ 0 ] s [ 0 ] = 1.5 s if the labels for the indices are specified, s = pd . Series ( np . random . randn ( 5 ), index = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) s [ 'a' ] s [ 0 ] s [ 'b' ] = 1.8 s s [ 2 ] = 2 s","title":"Series"},{"location":"archive/202203/lab5/#dataframe","text":"Creation from NumPy array df = pd . DataFrame ( np . random . randn ( 6 , 4 )) Indices and column names can be provided at creation. df = pd . DataFrame ( np . random . randn ( 6 , 4 ), index = list ( 'abcdef' ), columns = list ( 'ABCD' )) from a dict run the following code and understand the functions used. df2 = pd . DataFrame ({ 'A' : 1 , 'B' : pd . Timestamp ( '20190930' ), 'C' : pd . date_range ( '20190930' , periods = 4 ), 'D' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'E' : np . array ([ 3 ] * 4 , dtype = 'int32' ), 'F' : pd . Categorical ([ 'test' , 'train' , 'test' , 'train' ]), 'G' : 'foo' }) dtypes of a DataFrame can be viewed using df2.dtypes . In IPython, tab completion is enabled for column names and public attributes. Data display What do df.head(0) and df.tail() do? What happens if I use df.head(3) and df.tail(2) ? df.index displays the indices of a data frame. df.columns displays the columns of a data frame. df.describe() shows a quick statistical summary of each column of the data. Direct indexing to get a column, df [ 'A' ] df . A df[0] would not work. to select multiple columns, df [[ 'A' , 'B' ]] to get a slice of rows df [ 0 : 4 ] df [ 'a' : 'd' ] Is the indexing inclusive or exclusive? Selection by label With the following lines, identify how the function .loc[...] works df . loc [ 'a' ] df . loc [ 'a' : 'c' ] df . loc [[ 'a' , 'c' ]] df . loc [:, 'A' ] df . loc [:, [ 'A' , 'B' ]] df . loc [:, 'A' : 'C' ] df . loc [ 'a' : 'c' , [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], [ 'A' , 'B' ]] df . loc [[ 'a' , 'c' ], 'A' : 'C' ] df . loc [ 'a' : 'c' , 'A' : 'C' ] df . loc [ 'a' , 'A' ] df . loc [ 'a' , 'A' : 'C' ] df.at['a','A'] is equivalent to df.loc['a','A'] (only to get a scalar value) The object returned by a .loc is either a series (1-D), data frame (2-D), or scalar (single value). Selection by position .iloc and .iat work similarly as .loc and .at . The only difference is that, instead of the label of the row/column, we will use the position of the row/column. Find the equivalent usage of .iloc that provides the same outputs as the previous lines for .loc . Boolean indexing Investigate the differences in the outputs of the following lines: df [ df . A > 0 ] df [ df > 0 ] Filtering of a column can be done with .isin . df2 = df . copy () df2 [ 'E' ] = [ 'one' , 'one' , 'two' , 'three' , 'four' , 'three' ] df2 [ df2 [ 'E' ] . isin ([ 'two' , 'four' ])]","title":"DataFrame"},{"location":"archive/202203/lab5/#data-manipulation","text":"pandas library provides a lot of functions to manipulate data. Go to UCI datasets to download iris.data and iris.names from the Data Folder . Load iris.data as a data frame (Hint: iris.data is a CSV file) Update the column names based on iris.names . Calculate the mean, min, max, and standard deviation of each column. Create a new column called class value using the following code: df [ 'class value' ] = pd . factorize ( df [ 'class' ])[ 0 ] Investigate the output of pd.factorize . Group the data according to the class. (Hint: .groupby ) Identify the function to extract each group using the name of the class. Calculate the mean, min, max, and standard deviation of each column in each group. Produce a scatter plot for any two columns using matplotlib library. Identify the methods (at least 2) to loop through a data frame row by row.","title":"Data manipulation"},{"location":"archive/202203/lab6/","text":"Lab 6: Linear Regression Objectives To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library. Dataset Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data? Gradient descent on linear regression model with least squares method Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' ) Learning on linear regression model using scikit-learn scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' ) Multivariate linear regression Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Lab 6: Linear Regression"},{"location":"archive/202203/lab6/#lab-6-linear-regression","text":"","title":"Lab 6: Linear Regression"},{"location":"archive/202203/lab6/#objectives","text":"To develop the script to perform gradient descent on linear regression model with least squares method using Python. To train a linear regression model with least squares method using the scikit-learn Python library.","title":"Objectives"},{"location":"archive/202203/lab6/#dataset","text":"Throughout this lab we will be using the data for 442 diabetes patients. The data can be import from the sklearn library. from sklearn import datasets diabetes = datasets . load_diabetes () diabetes contains the keys of data , target , DESCR , feature_names , data_filename , and target_filename . The description of the diabetes dataset is given by print ( diabetes . DESCR ) Task : What are the keys for the input data and the output/target data?","title":"Dataset"},{"location":"archive/202203/lab6/#gradient-descent-on-linear-regression-model-with-least-squares-method","text":"Load the dataset as described in the Dataset Section. from sklearn import datasets diabetes = datasets . load_diabetes () Convert the dataset into a pandas DataFrame. import pandas as pd dt = pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ) y = pd . DataFrame ( diabetes . target , columns = [ 'target' ]) In machine learning, the common practice is to split the dataset into training data and testing data (some also include the validation data). The testing data is not used during the training phase, but only after training to evaluate the model. The split is normally done such that the training data has a larger portion than the testing data. In this case, let's use a 80-20 split for the training data and testing data. sklearn provides a function to split the train-test data given a percentage. from sklearn.model_selection import train_test_split Task : How do you use train_test_split to split the data and target into 80-20 proportion? (Define the training features as X_train , training target as y_train , testing features as X_test , testing target as y_test .) Before we train the data, a few functions need to be defined. We will start with defining the model of a simple regression line, i.e. \\(y = mx + c\\) . Define the function name as model with 3 inputs x , m , and c . The function should return the value of y . The next function to define is the cost function, i.e. \\[J = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Name the function as cost with 2 inputs y as the array of target values and yh as the array of predicted target values. The function returns a scalar value of the cost. Assume the arrays are pandas Series. We also need to define the function to calculate the derivatives of the cost with respect to each parameter. Name the function as derivatives with 3 inputs x as the array of input values, y as the array of of target values, and yh as the array of predicted target values. The function returns a dict object with keys m to hold the value of \\(\\frac{\\partial J}{\\partial m}\\) and c the value of \\(\\frac{\\partial J}{\\partial c}\\) . Assume the arrays are pandas Series. \\[\\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i (y_i - \\hat{y}_i)\\] \\[\\frac{\\partial J}{\\partial c} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\] We will use bmi as our input feature. Define the initial values for the parameters. learningrate = 0.1 m = [] c = [] J = [] m . append ( 0 ) c . append ( 0 ) J . append ( cost ( y_train [ 'target' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])))) Define the termination conditions. J_min = 0.01 del_J_min = 0.0001 max_iter = 10000 Variable Termination condition J_min The algorithm terminates when the cost is less than this value. del_J_min The algorithm terminates when the proportion of change in cost to the current cost is less than this value. max_iter The algorithm terminates when the number of iteration exceeds this value. Now develop the code to perform gradient descent. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m\\) and \\(c\\) ; append the new values to the arrays m and c . Calculate \\(J\\) ; append the value to the array J . Repeat from the beginning. To provide feedback during each iteration, import sys , and add the following lines as the last lines in the loop. print ( '.' , end = '' ) sys . stdout . flush () To provide visual display for each iteration, import matplotlib.pyplot as plt . Add these lines before the loop. plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . title ( 'Training data' ) plt . xlabel ( 'BMI' ) line = None Add the following lines as the last lines in the loop. if line : line [ 0 ] . remove () line = plt . plot ( X_train [ 'bmi' ], X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), '-' , color = 'green' ) plt . pause ( 0.001 ) Outside of the loop, add the following lines to display the results. y_train_pred = X_train [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) y_test_pred = X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])) print ( ' \\n Algorithm terminated with' ) print ( f ' { len ( J ) } iterations,' ) print ( f ' m { m [ - 1 ] } ' ) print ( f ' c { c [ - 1 ] } ' ) print ( f ' training cost { J [ - 1 ] } ' ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' testing cost { testcost } ' ) Test the result on the testing data. plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], \\ X_test [ 'bmi' ] . apply ( lambda x : model ( x , m [ - 1 ], c [ - 1 ])), \\ '-' , color = 'green' ) plt . title ( 'Testing data' )","title":"Gradient descent on linear regression model with least squares method"},{"location":"archive/202203/lab6/#learning-on-linear-regression-model-using-scikit-learn","text":"scikit-learn provides model and functions to perform linear regression easily. from sklearn import linear_model Create a linear regression model. regrmodel = linear_model . LinearRegression () Train the model with the training data. regrmodel . fit ( X_train [[ 'bmi' ]], y_train [ 'target' ]) regrmodel is now a trained model. To get the predicted \\(y\\) given a set of \\(x\\) values, y_train_pred = regrmodel . predict ( X_train [[ 'bmi' ]]) y_test_pred = regrmodel . predict ( X_test [[ 'bmi' ]]) As we are using pandas data structures, we need to convert the predicted value to Series and align the indices with the target Series. y_train_pred = pd . Series ( y_train_pred ) y_train_pred . index = y_train . index y_test_pred = pd . Series ( y_test_pred ) y_test_pred . index = y_test . index Display the information of the fitted model. print ( 'sklearn' ) print ( f ' m { regrmodel . coef_ } ' ) print ( f ' c { regrmodel . intercept_ } ' ) traincost = cost ( y_train [ 'target' ], y_train_pred ) testcost = cost ( y_test [ 'target' ], y_test_pred ) print ( f ' training cost: { traincost } ' ) print ( f ' testing cost: { testcost } ' ) Display the result of prediction together with the target values. plt . figure () plt . scatter ( X_train [ 'bmi' ], y_train [ 'target' ], color = 'red' ) plt . plot ( X_train [ 'bmi' ], y_train_pred , '-' , color = 'green' ) plt . title ( 'Training data (sklearn)' ) plt . xlabel ( 'BMI' ) plt . figure () plt . scatter ( X_test [ 'bmi' ], y_test [ 'target' ], color = 'red' ) plt . plot ( X_test [ 'bmi' ], y_test_pred , '-' , color = 'green' ) plt . title ( 'Testing data (sklearn)' ) plt . xlabel ( 'BMI' )","title":"Learning on linear regression model using scikit-learn"},{"location":"archive/202203/lab6/#multivariate-linear-regression","text":"Linear regression can also be applied with multiple inputs. With \\(k\\) inputs, the linear regression equation is given as \\[\\hat{y} = \\beta + m_1x_1 + m_2x_2 + \\cdots + m_kx_k\\] With one input, the resultant function is a line; with two inputs, the resultant function is a plane; with more inputs, it's a hyperplane. Gradient descent for multivariate linear regression is performed with the same approach. Check termination conditions, if any condition fulfilled, the algorithm is terminated. Calculate derivatives. Update \\(m_1\\) , \\(m_2\\) , ..., \\(m_k\\) and \\(\\beta\\) . Calculate \\(J\\) . Repeat from the beginning. For multivariate linear regression, the least squares cost is given by \\( \\(J = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \\) The derivative with respect to \\(\\beta\\) \\( \\(\\frac{\\partial J}{\\partial \\beta} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\) \\) The derivatives with respect to \\(m_j\\) \\( \\(\\frac{\\partial J}{\\partial m_j} = -\\frac{2}{n} \\sum_{i=1}^{n} x_{ji}(y_i - \\hat{y}_i)\\) \\) Using scikit-learn functions, multivariate linear regression can be achieved by providing the input features to the function. We will now use the bmi and blood pressure bp as the input features to predict the target. Copy the code block from the previous section and change X_...[['bmi']] to X_...[['bmi','bp']] except for the plotting part. For the graphical visualisation (plotting) part, use the following code to plot 3d graphs: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train [ 'target' ], color = 'red' ) ax . scatter ( X_train [ 'bmi' ], X_train [ 'bp' ], y_train_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Training data (sklearn multivariate)' ) fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test [ 'target' ], color = 'red' ) ax . scatter ( X_test [ 'bmi' ], X_test [ 'bp' ], y_test_pred , color = 'green' ) ax . set_xlabel ( 'BMI' ) ax . set_ylabel ( 'Blood pressure' ) ax . set_title ( 'Testing data (sklearn multivariate)' )","title":"Multivariate linear regression"},{"location":"archive/202203/lab7/","text":"Lab 7: k Nearest Neighbours (KNN) Objective To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression. Datasets The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset? KNN KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression. Classification Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis. Regression Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Lab 7: *k* Nearest Neighbours (KNN)"},{"location":"archive/202203/lab7/#lab-7-k-nearest-neighbours-knn","text":"","title":"Lab 7: k Nearest Neighbours (KNN)"},{"location":"archive/202203/lab7/#objective","text":"To perform k nearest neighbours algorithm with scikit-learn Python library for classification and regression.","title":"Objective"},{"location":"archive/202203/lab7/#datasets","text":"The iris dataset will be used for classification, and the diabetes dataset for regression. from sklearn import datasets import pandas as pd iris = datasets . load_iris () iris = { 'attributes' : pd . DataFrame ( iris . data , columns = iris . feature_names ), 'target' : pd . DataFrame ( iris . target , columns = [ 'species' ]), 'targetNames' : iris . target_names } diabetes = datasets . load_diabetes () diabetes = { 'attributes' : pd . DataFrame ( diabetes . data , columns = diabetes . feature_names ), 'target' : pd . DataFrame ( diabetes . target , columns = [ 'diseaseProgression' ]) } Split the datasets into 80-20 for train-test proportion. from sklearn.model_selection import train_test_split for dt in [ iris , diabetes ]: x_train , x_test , y_train , y_test = train_test_split ( dt [ 'attributes' ], dt [ 'target' ], test_size = 0.2 , random_state = 1 ) dt [ 'train' ] = { 'attributes' : x_train , 'target' : y_train } dt [ 'test' ] = { 'attributes' : x_test , 'target' : y_test } Note : Be reminded that random_state is used to reproduce the same \"random\" split of the data whenever the function is called. To produce randomly splitted data every time the function is called, remove the random_state argument. Task : How do we access the training input data for the iris dataset?","title":"Datasets"},{"location":"archive/202203/lab7/#knn","text":"KNN algorithms are provided by the scikit-learn Python library as the class sklearn.neighbors.KNeighborsClassifier for classification, and sklearn.neighbors.KNeighborsRegressor for regression.","title":"KNN"},{"location":"archive/202203/lab7/#classification","text":"Import the class for KNN classifier. from sklearn.neighbors import KNeighborsClassifier Instantiate an object of KNeighborsClassifier class with k = 5. knc = KNeighborsClassifier ( 5 ) Train the classifier with the training data. We will use the sepal length (cm) and sepal width (cm) (the first and second columns) as the attributes for now. input_columns = iris [ 'attributes' ] . columns [: 2 ] . tolist () x_train = iris [ 'train' ][ 'attributes' ][ input_columns ] y_train = iris [ 'train' ][ 'target' ] . species knc . fit ( x_train , y_train ) .predict function is used to predict the species of the testing data. x_test = iris [ 'test' ][ 'attributes' ][ input_columns ] y_test = iris [ 'test' ][ 'target' ] . species y_predict = knc . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knc . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.colors import ListedColormap Prepare the colormaps. colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculate the decision boundaries. import numpy as np x_min = iris [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure ( figsize = [ 12 , 8 ]) plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = cm_dark , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Classification"},{"location":"archive/202203/lab7/#regression","text":"Import the class for KNN regressor. from sklearn.neighbors import KNeighborsRegressor Instantiate an object of KNeighborsRegressor class with k = 5. knr = KNeighborsRegressor ( 5 ) Train the regressor with the training data. We will use the age and bmi as the attributes for now. input_columns = [ 'age' , 'bmi' ] x_train = diabetes [ 'train' ][ 'attributes' ][ input_columns ] y_train = diabetes [ 'train' ][ 'target' ] . diseaseProgression knr . fit ( x_train , y_train ) .predict function is used to predict the disease progression of the testing data. x_test = diabetes [ 'test' ][ 'attributes' ][ input_columns ] y_test = diabetes [ 'test' ][ 'target' ] . diseaseProgression y_predict = knr . predict ( x_test ) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( y_test , y_predict )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. print ( f 'Accuracy: { knr . score ( x_test , y_test ) : .4f } ' ) Visualisation Import the matplotlib.pyplot library and the colormaps from the matplotlib library. import matplotlib.pyplot as plt from matplotlib import cm Prepare the colormaps. dia_cm = cm . get_cmap ( 'Reds' ) Calculate the decision boundaries. import numpy as np x_min = diabetes [ 'attributes' ][ input_columns [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_columns [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_columns [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_columns [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = knr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision boundary. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( x_train [ input_columns [ 0 ]], x_train [ input_columns [ 1 ]], c = y_train , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( x_test [ input_columns [ 0 ]], x_test [ input_columns [ 1 ]], c = y_test , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_columns [ 0 ]) plt . ylabel ( input_columns [ 1 ]) plt . legend () plt . colorbar () Task : Create a loop to compare the accuracy of the prediction at different value of k. The comparison should be shown in a graph with k as the horizontal axis and accuracy as the vertical axis.","title":"Regression"},{"location":"archive/202203/lab8/","text":"Lab 8: Decision Tree Objective To perform CART decision tree learning algorithm using the scikit-learn Python library. Datasets The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion. Decision tree Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression. Classification Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code. Overfitting Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation. Regression Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy. Visualisation of decision surface This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar () Overfitting Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Lab 8: Decision Tree"},{"location":"archive/202203/lab8/#lab-8-decision-tree","text":"","title":"Lab 8: Decision Tree"},{"location":"archive/202203/lab8/#objective","text":"To perform CART decision tree learning algorithm using the scikit-learn Python library.","title":"Objective"},{"location":"archive/202203/lab8/#datasets","text":"The same iris and diabetes datasets used in Lab 7 are used here. Load the datasets and split them into 80-20 train-test proportion.","title":"Datasets"},{"location":"archive/202203/lab8/#decision-tree","text":"Decision tree models and algorithms are provided by the scikit-learn as the class sklearn.tree.DecisionTreeClassifier for classification, and sklearn.tree.DecisionTreeRegressor for regression.","title":"Decision tree"},{"location":"archive/202203/lab8/#classification","text":"Import the class for decision tree classifier. from sklearn.tree import DecisionTreeClassifier Instantiate an object of DecisionTreeClassifier class with gini impurity as the split criterion. dtc = DecisionTreeClassifier ( criterion = 'gini' ) Train the classifier with the training data. We will use all the input attributes. dtc . fit ( iris [ 'train' ][ 'attributes' ], iris [ 'train' ][ 'target' ]) .predict function is used to predict the species of the testing data. predicts = dtc . predict ( iris [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( iris [ 'test' ][ 'target' ] . species , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtc . score ( iris [ 'test' ][ 'attributes' ], iris [ 'test' ][ 'target' ] . species ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtc , feature_names = iris [ 'attributes' ] . columns . tolist (), class_names = iris [ 'targetNames' ], filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeClassifier(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Classification"},{"location":"archive/202203/lab8/#visualisation-of-decision-surface","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, sepal length and sepal width, i.e. the first two columns. Instantiate the classifier without defining the maximum depth and train the model. dtc = DecisionTreeClassifier () input_cols = iris [ 'train' ][ 'attributes' ] . columns [: 2 ] . tolist () dtc . fit ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtc , feature_names = input_cols , class_names = iris [ 'targetNames' ], filled = True , rounded = True ) plt . savefig ( 'classificationDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm from matplotlib.colors import ListedColormap colormap = cm . get_cmap ( 'tab20' ) cm_dark = ListedColormap ( colormap . colors [:: 2 ]) cm_light = ListedColormap ( colormap . colors [ 1 :: 2 ]) Calculating the decision surface. import numpy as np x_min = iris [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = iris [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = iris [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = iris [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtc . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface. plt . figure () plt . pcolormesh ( xx , yy , z , cmap = cm_light ) Plot the training and testing data. plt . scatter ( iris [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'train' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Training data' , edgecolor = 'black' , linewidth = 1 ) plt . scatter ( iris [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], iris [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = iris [ 'test' ][ 'target' ] . species , cmap = cm_dark , s = 200 , label = 'Testing data' , edgecolor = 'black' , linewidth = 1 , marker = '*' ) train_acc = dtc . score ( iris [ 'train' ][ 'attributes' ][ input_cols ], iris [ 'train' ][ 'target' ] . species ) test_acc = dtc . score ( iris [ 'test' ][ 'attributes' ][ input_cols ], iris [ 'test' ][ 'target' ] . species ) plt . title ( f 'training: { train_acc : .3f } , testing: { test_acc : .3f } ' ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () You may not be able to see anything on one of the graph of the decision tree because the figure size is set to be larger than the screen size. However, the tree is saved to a png file in the same folder as your code.","title":"Visualisation of decision surface"},{"location":"archive/202203/lab8/#overfitting","text":"Now, instantiate a decision tree classifier of max_depth=3 and train it with the two input attributes used in the previous section, sepal length and sepal width. Plot the decision surface for this classifier after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202203/lab8/#regression","text":"Import the class for decision tree regressor. from sklearn.tree import DecisionTreeRegressor Instantiate an object of DecisionTreeRegressor class. dtr = DecisionTreeRegressor () Train the classifier with the training data. We will use all the input attributes. dtr . fit ( diabetes [ 'train' ][ 'attributes' ], diabetes [ 'train' ][ 'target' ]) .predict function is used to predict the disease progression of the testing data. predicts = dtr . predict ( diabetes [ 'test' ][ 'attributes' ]) Comparing the predicted value and the target value of the test data. print ( pd . DataFrame ( list ( zip ( diabetes [ 'test' ][ 'target' ] . diseaseProgression , predicts )), columns = [ 'target' , 'predicted' ])) Calculate the accuracy of the predicted value. accuracy = dtr . score ( diabetes [ 'test' ][ 'attributes' ], diabetes [ 'test' ][ 'target' ] . diseaseProgression ) print ( f 'Accuracy: { accuracy : .4f } ' ) Decision tree visualisation Import the matplotlib.pyplot library and the function to visualise the tree. import matplotlib.pyplot as plt from sklearn.tree import plot_tree Visualise the decision tree model. plt . figure ( figsize = [ 10 , 10 ]) tree = plot_tree ( dtr , feature_names = diabetes [ 'attributes' ] . columns . tolist (), filled = True , rounded = True ) The maximum depth of a decision tree can be defined by adding the max_depth=... argument to the DecisionTreeRegressor(...) object instantiation. To allow unlimited maximum depth, pass max_depth=None . Task : Create a loop to compare the accuracy of the prediction with different maximum depths. Use 1, 2, 3, 5, 7, and 20 as the maximum depths. In every iteration, you should calculate both the accuracy on the training data and the accuracy on the testing data. The comparison should be shown in a graph with max_depth as the horizontal axis and accuracy as the vertical axis. Two lines should be displayed on the graph with one line for training accuracy and the other testing accuracy.","title":"Regression"},{"location":"archive/202203/lab8/#visualisation-of-decision-surface_1","text":"This section explains the method to visualise a decision tree on a graph. To do so we will focus on using two input attributes, age and bmi . Instantiate the classifier without defining the maximum depth and train the model. dtr = DecisiontTreeRegressor () input_cols = [ 'age' , 'bmi' ] dtr . fit ( diabetes [ 'train' ][ 'attributes' ][ input_cols ], diabetes [ 'train' ][ 'target' ] . diseaseProgression ) Plot the decision tree. plt . figure ( figsize = [ 50 , 50 ]) plot_tree ( dtr , feature_names = input_cols , filled = True , rounded = True ) plt . savefig ( 'regressionDecisionTreeWithNoMaxDepth.png' ) Prepare the colormaps. from matplotlib import cm dia_cm = cm . get_cmap ( 'Reds' ) Create the decision surface. import numpy as np x_min = diabetes [ 'attributes' ][ input_cols [ 0 ]] . min () x_max = diabetes [ 'attributes' ][ input_cols [ 0 ]] . max () x_range = x_max - x_min x_min = x_min - 0.1 * x_range x_max = x_max + 0.1 * x_range y_min = diabetes [ 'attributes' ][ input_cols [ 1 ]] . min () y_max = diabetes [ 'attributes' ][ input_cols [ 1 ]] . max () y_range = y_max - y_min y_min = y_min - 0.1 * y_range y_max = y_max + 0.1 * y_range xx , yy = np . meshgrid ( np . arange ( x_min , x_max , .01 * x_range ), np . arange ( y_min , y_max , .01 * y_range )) z = dtr . predict ( list ( zip ( xx . ravel (), yy . ravel ()))) z = z . reshape ( xx . shape ) Plot the decision surface plt . figure () plt . pcolormesh ( xx , yy , z , cmap = dia_cm ) Plot the training and testing data. plt . scatter ( diabetes [ 'train' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'train' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'train' ][ 'target' ] . diseaseProgression , label = 'Training data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . scatter ( diabetes [ 'test' ][ 'attributes' ][ input_cols [ 0 ]], diabetes [ 'test' ][ 'attributes' ][ input_cols [ 1 ]], c = diabetes [ 'test' ][ 'target' ] . diseaseProgression , marker = '*' , label = 'Testing data' , cmap = dia_cm , edgecolor = 'black' , linewidth = 1 , s = 150 ) plt . xlabel ( input_cols [ 0 ]) plt . ylabel ( input_cols [ 1 ]) plt . legend () plt . colorbar ()","title":"Visualisation of decision surface"},{"location":"archive/202203/lab8/#overfitting_1","text":"Now, instantiate a decision tree regressor of max_depth=3 and train it with the two input attributes used in the previous section, age and bmi . Plot the decision surface for this regressor after the training. Compare the decision surface, training accuracy, and testing accuracy between this model and the model in the previous section. The previous model shows a situation of overfitting. Though the training accuracy of this model is lower than that of the previous one, the testing accuracy is higher than the previous one. That shows a higher level of generalisation.","title":"Overfitting"},{"location":"archive/202203/lab9/","text":"Lab 9: Clustering Objective To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library. Note Suggestion: use numpy library to simplify the operation. k -means clustering using basic Python The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' ) Dataset The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ]) Initialisation The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 ) Cluster the points The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids ) Update the centroids The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups ) Termination condition The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much. Visualisation Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical? k -means clustering using scikit-learn Python library The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ... Dataset We will use the exact same data generation as the previous section. k-means model The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans ) Training The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... ) Cluster the points The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... ) Visualisation The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Lab 9: Clustering"},{"location":"archive/202203/lab9/#lab-9-clustering","text":"","title":"Lab 9: Clustering"},{"location":"archive/202203/lab9/#objective","text":"To implement k -means clustering algorithm using basic Python. To perform k -means clustering algorithm with scikit-learn Python library.","title":"Objective"},{"location":"archive/202203/lab9/#note","text":"Suggestion: use numpy library to simplify the operation.","title":"Note"},{"location":"archive/202203/lab9/#k-means-clustering-using-basic-python","text":"The following code structure will be used for this section: # import libraries ... # functions ## function to take input of data and number of clusters, return centroids and other data def get_random_centroids ( data_points , n_centroids = 2 ): pass ## function to group data according to centroids def group_to_centroids ( data_points , centroids ): pass ## function to calculate centroids from grouped data def find_centroids ( data_points , groups ): pass # generate dataset ... # identify initial centroids ... # repeat until centroids stabilise while ... : ## group data to centroids ... ## update centroids ... print ( 'terminated' )","title":"k-means clustering using basic Python"},{"location":"archive/202203/lab9/#dataset","text":"The code in this subsection will populate # generate dataset ... We will create a dataset of 200 with 2 input features and 4 clusters. from sklearn.datasets import make_blobs data = make_blobs ( n_samples = 200 , n_features = 2 , centers = 4 , cluster_std = 1.6 , random_state = 50 ) data is an array of two elements. First element contains the data points, and second element contains the index of the cluster. points = data [ 0 ] Plot the data on a scatter graph with colour representing the cluster of the points. import matplotlib.pyplot as plt plt . scatter ( data [ 0 ][:, 0 ], data [ 0 ][:, 1 ], c = data [ 1 ])","title":"Dataset"},{"location":"archive/202203/lab9/#initialisation","text":"The initialisation for k -means clustering algorithm involves the identification of k random points from the dataset. In the get_random_centroids function, pass the dataset and the number of centroids to be identified as the input arguments. In the body of the function, randomly identify the centroids from the dataset. The function should return two outputs, the randomly identified centroids and the dataset without the centroids. Update your code with the following snippet: # identify initial centroids centroids , others = get_random_centroids ( points , 4 )","title":"Initialisation"},{"location":"archive/202203/lab9/#cluster-the-points","text":"The group_to_centroids function takes two inputs, the data points to be clustered and the centroids to cluster to. In the group_to_centroids function, calculate the distance of every point from each centroid. Then identify the centroid each point should be clustered to. The function should return the index of the centroid each point is clustered to. Update your code with the following snippet: ## group data to centroids groups = group_to_centroids ( others , centroids )","title":"Cluster the points"},{"location":"archive/202203/lab9/#update-the-centroids","text":"The find_centroids function takes two inputs, the data points and the index of the centroid each point is clustered to (i.e. output of group_to_centroids ) The find_centroids function calculates and returns the new set of centroids based on the clustered data points. Update your code with the following snippet: ## update centroids centroids = find_centroids ( others , groups )","title":"Update the centroids"},{"location":"archive/202203/lab9/#termination-condition","text":"The clustering algorithm should terminate when the centroids stabilise, i.e. do not change much.","title":"Termination condition"},{"location":"archive/202203/lab9/#visualisation","text":"Update your code according to the following sample to visualise the centroids and clusters at every iteration. fig = plt . figure () ax = fig . add_subplot ( 111 ) # repeat until centroids stabilise while ... : ## group data to centroids groups = group_to_centroids ( others , centroids ) ## update centroids centroids = find_centroids ( others , groups ) ## visualise current clusters and centroids ax . clear () ax . scatter ( others [:, 0 ], others [:, 1 ], c = groups ) ax . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = '*' , c = 'k' ) ## pause for one second plt . pause ( 1 ) Are Figure 1 (originally generated clusters) and Figure 2 (calculated clusters) identical?","title":"Visualisation"},{"location":"archive/202203/lab9/#k-means-clustering-using-scikit-learn-python-library","text":"The following code structure will be used for this section: # import libraries ... # generate dataset ... # initialise the k-means model ... # train the k-means model ... # identify the cluster of each point ... # visualise the result ...","title":"k-means clustering using scikit-learn Python library"},{"location":"archive/202203/lab9/#dataset_1","text":"We will use the exact same data generation as the previous section.","title":"Dataset"},{"location":"archive/202203/lab9/#k-means-model","text":"The k -means model is provided by sklearn.cluster.KMeans . from sklearn.cluster import KMeans Initialise the k -means model with 4 clusters using KMeans . (Check the documentation to identify the usage of KMeans )","title":"k-means model"},{"location":"archive/202203/lab9/#training","text":"The k -means model initialised need to be trained with the data. The training is executed using sklearn.cluster.KMeans.fit . (Identify the input argument(s)) kmeans . fit ( ... )","title":"Training"},{"location":"archive/202203/lab9/#cluster-the-points_1","text":"The trained model can be used to cluster the points to their respective cluster using sklearn.cluster.KMeans.fit_predict . (Identify the input argument(s)) y_km = kmeans . fit_predict ( ... )","title":"Cluster the points"},{"location":"archive/202203/lab9/#visualisation_1","text":"The visualisation of the result can be achieved with the following code: plt . figure () plt . scatter ( points [:, 0 ], points [:, 1 ], c = y_km ) plt . scatter ( kmeans . cluster_centers_ [:, 0 ], kmeans . cluster_centers_ [:, 1 ], c = 'k' ) Task : Compare the results of the two methods, are they similar?","title":"Visualisation"}]}